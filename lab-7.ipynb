{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sivakumar-Navaneeth/Speech_and_Language_Processing_LAB/blob/main/lab-7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LAB-A-5\n",
        "\n",
        "### Author\n",
        "\n",
        "- [Navaneeth Sivakumar - 21BAI1302](https://github.com/Sivakumar-Navaneeth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xj_12eO93nvi",
        "outputId": "b9a8a645-3c5d-407b-9cd0-96f0f361d2d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': '5 stars', 'score': 0.8546809554100037}]\n",
            "[{'label': '1 star', 'score': 0.6346080899238586}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "c1 = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "print(c1('I love you'))\n",
        "print(c1('I hate you'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Text Generation (GPT2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEXSrlma3nvj",
        "outputId": "b4a575b7-3cb2-4985-8aa0-a515158967dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': \"The movie was released in Japan on May 7, 2016.\\n\\nThe movie is set in the same universe as the original manga, and is based on the manga's story.\\n\\nThe movie is set in the same universe as the original manga\"}]\n",
            "[{'generated_text': 'The movie was directed by David S. Goyer and directed by Marc Benioff and written by Steve Broussard (The Amazing Spider-Man, The Wicker Man, Mad Max: Fury Road).\\n\\n[Via IGN]'}]\n"
          ]
        }
      ],
      "source": [
        "c2 = pipeline('text-generation', model='gpt2')\n",
        "print(c2('The movie was', max_length=50, do_sample=False))\n",
        "print(c2('The movie was', max_length=50, do_sample=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Fill Mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84AZv52-3nvk",
        "outputId": "8c5c9c90-06a3-4e05-fc5f-7d838933a251"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilroberta-base and revision ec58a5b (https://huggingface.co/distilbert/distilroberta-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'score': 0.08974882960319519, 'token': 6587, 'token_str': ' terrible', 'sequence': 'The movie was terrible and I didnt like it'}, {'score': 0.05882135033607483, 'token': 6269, 'token_str': ' funny', 'sequence': 'The movie was funny and I didnt like it'}, {'score': 0.05695611983537674, 'token': 15305, 'token_str': ' boring', 'sequence': 'The movie was boring and I didnt like it'}, {'score': 0.05357389524579048, 'token': 36331, 'token_str': ' cheesy', 'sequence': 'The movie was cheesy and I didnt like it'}, {'score': 0.04862167686223984, 'token': 11522, 'token_str': ' awful', 'sequence': 'The movie was awful and I didnt like it'}]\n",
            "[{'score': 0.12370163947343826, 'token': 5500, 'token_str': ' fantastic', 'sequence': 'The movie was fantastic and I loved it'}, {'score': 0.09648788720369339, 'token': 6344, 'token_str': ' awesome', 'sequence': 'The movie was awesome and I loved it'}, {'score': 0.0831378698348999, 'token': 6269, 'token_str': ' funny', 'sequence': 'The movie was funny and I loved it'}, {'score': 0.07116565108299255, 'token': 14353, 'token_str': ' terrific', 'sequence': 'The movie was terrific and I loved it'}, {'score': 0.0701359435915947, 'token': 2770, 'token_str': ' amazing', 'sequence': 'The movie was amazing and I loved it'}]\n"
          ]
        }
      ],
      "source": [
        "c3 = pipeline(task='fill-mask')\n",
        "print(c3('The movie was <mask> and I didnt like it'))\n",
        "print(c3('The movie was <mask> and I loved it'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Text Generation (DistilGPT2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "islY0VPT3nvk",
        "outputId": "59a458d1-41c0-410b-fde2-b296cb5d7663"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': 'The movie was a hit in the United States and was a hit in the United Kingdom.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}]\n",
            "[{'generated_text': 'The movie was filmed in China in 2002 through \"China Live.\" It was never made in the North by China.\\n\\n\\n\\n\\nIn July 2011, Chinese military spokesman Zifai Xinhua, quoted a military statement said about the film as'}]\n"
          ]
        }
      ],
      "source": [
        "c4 = pipeline('text-generation', model='distilgpt2')\n",
        "print(c4('The movie was', max_length=50, do_sample=False))\n",
        "print(c4('The movie was', max_length=50, do_sample=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. Name Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaU3TN_c3nvk",
        "outputId": "62535967-0de3-44ca-b112-c8b487ccb1ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'entity': 'B-PER', 'score': 0.9987638, 'index': 4, 'word': 'John', 'start': 11, 'end': 15}, {'entity': 'I-PER', 'score': 0.99894613, 'index': 5, 'word': 'Do', 'start': 16, 'end': 18}, {'entity': 'I-PER', 'score': 0.98036706, 'index': 6, 'word': '##e', 'start': 18, 'end': 19}, {'entity': 'B-LOC', 'score': 0.99924135, 'index': 11, 'word': 'New', 'start': 34, 'end': 37}, {'entity': 'I-LOC', 'score': 0.9993963, 'index': 12, 'word': 'York', 'start': 38, 'end': 42}]\n",
            "[{'entity': 'B-PER', 'score': 0.9988097, 'index': 4, 'word': 'John', 'start': 11, 'end': 15}, {'entity': 'I-PER', 'score': 0.99899167, 'index': 5, 'word': 'Do', 'start': 16, 'end': 18}, {'entity': 'I-PER', 'score': 0.98145264, 'index': 6, 'word': '##e', 'start': 18, 'end': 19}, {'entity': 'B-LOC', 'score': 0.9992136, 'index': 11, 'word': 'New', 'start': 34, 'end': 37}, {'entity': 'I-LOC', 'score': 0.99944216, 'index': 12, 'word': 'York', 'start': 38, 'end': 42}, {'entity': 'B-ORG', 'score': 0.9976279, 'index': 17, 'word': 'Google', 'start': 57, 'end': 63}]\n"
          ]
        }
      ],
      "source": [
        "c5 = pipeline('ner', model='dslim/bert-base-NER')\n",
        "print(c5('My name is John Doe and I live in New York'))\n",
        "print(c5('My name is John Doe and I live in New York and I work at Google'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6. Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anO-JnUG3nvk",
        "outputId": "a55f7c8c-f37f-45ac-e0de-e04bb8eff8e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at mrm8488/bert-tiny-finetuned-squadv2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'score': 0.07067025452852249, 'start': 11, 'end': 42, 'answer': 'John Doe and I live in New York'}\n",
            "{'score': 0.10650753974914551, 'start': 34, 'end': 42, 'answer': 'New York'}\n"
          ]
        }
      ],
      "source": [
        "c6 = pipeline('question-answering', model='mrm8488/bert-tiny-finetuned-squadv2')\n",
        "print(c6(context='My name is John Doe and I live in New York', question='What is my name?'))\n",
        "print(c6(context='My name is John Doe and I live in New York', question='Where do I live?'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7. Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0TmfaVx3nvk",
        "outputId": "bb4593ec-0bfc-4838-d27a-8adc2df7e9b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'summary_text': ' The movie was great and I loved it. The actors were amazing and the story was very interesting .'}]\n"
          ]
        }
      ],
      "source": [
        "c7 = pipeline('summarization', model = 'sshleifer/distilbart-cnn-12-6', min_length = 2, max_length=23)\n",
        "print(c7('The movie was great and I loved it. The actors were amazing and the story was very interesting. I would recommend it to everyone. I will watch it again. I loved it. It was one wholesome experience.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 8. Translation (English to French)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgOrwX4m3nvl",
        "outputId": "d5b3c551-b6b9-4012-e234-e898a31ff246"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'translation_text': \"Le film était génial et j'adorais ça.\"}]\n"
          ]
        }
      ],
      "source": [
        "c8 = pipeline('translation_en_to_fr', model='Helsinki-NLP/opus-mt-en-fr')\n",
        "print(c8('The movie was great and I loved it.'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
